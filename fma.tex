% -----------------------------------------------
% Template for ISMIR Papers
% 2017 version, based on previous ISMIR templates

% Requirements :
% * 6+n page length maximum
% * 4MB maximum file size
% * Copyright note must appear in the bottom left corner of first page
% * Clearer statement about citing own work in anonymized submission
% (see conference website for additional details)
% -----------------------------------------------

% Anonymity: authors, GitHub link

\documentclass{article}
\usepackage{ismir,amsmath,cite,url}
\usepackage{graphicx} \graphicspath{{figs/}}
\usepackage{color}

\usepackage[utf8]{inputenc}
\usepackage[bookmarks=false,colorlinks=true,allcolors=blue]{hyperref}
\usepackage{booktabs}
%\usepackage[para,online,flushleft]{threeparttable}
\usepackage{threeparttable}
%\usepackage{footnote} \makesavenoteenv{table}
\usepackage{rotating,tabularx}

\title{FMA: A Dataset For Music Analysis}

\oneauthor
{Kirell Benzi, \hspace{.2cm} Michaël Defferrard, \hspace{.2cm} Pierre Vandergheynst, \hspace{.2cm} Xavier Bresson}
 {{\tt michael.defferrard@epfl.ch, kirell.benzi@epfl.ch} \\
  {\tt pierre.vandergheynst@epfl.ch, xavier.bresson@epfl.ch} \\
  LTS2, EPFL, Lausanne, Switzerland}

%\threeauthors
%  {First Author} {Affiliation1 \\ {\tt author1@ismir.edu}}
%  {Second Author} {\bf Retain these fake authors in\\\bf submission to preserve the formatting}
%  {Third Author} {Affiliation3 \\ {\tt author3@ismir.edu}}

%% To make customize author list in Creative Common license, uncomment and customize the next line
\def\authorname{Kirell Benzi, Michaël Defferrard, Pierre Vandergheynst, Xavier Bresson}

\sloppy % please retain sloppy command for improved formatting
\begin{document}
\maketitle

% about 150-200 words.
\begin{abstract}
We present a new music dataset that can be used for several music analysis tasks. Our major goal is to go beyond the existing limitations of available music datasets, which are either the small size of datasets with raw audio tracks, the availability and legality of the music data, or the lack of meta-data for artists analysis or song ratings for recommender systems. Existing datasets such as GTZAN, TagATune, and Million Song suffer from the previous limitations. It is however essential to establish such benchmark datasets to advance the field of music analysis, like the ImageNet dataset which made possible the large success of deep learning techniques in computer vision. In this paper, we introduce the Free Music Archive (FMA) which contains 77,643 songs and 68 genres spanning 26.9 days of song listening and meta-data including artist name, song title, music genre, and track counts. For research purposes, we define two additional datasets from the original one: a small genre-balanced dataset of 4,000 song data and 10 genres compassing 33.3 hours of raw audio and a medium genre-unbalanced dataset of 14,511 data and 20 genres offering 5.1 days of track listening, both datasets come with meta-data and Echonest audio features. For all datasets, we provide a train-test splitting for future algorithms' comparisons.
\end{abstract}


The released dataset is composed of 100,000 tracks, xx artists, xx albums, xx hours of audio (xx GiB), 164 genres, all under permissive Creative Commons licenses.

The dataset comes with a collection of meta-data such as the song titles, artists and albums, together with full-length audio and a set of pre-computed features as well as a train/test split.

Content-based music information retrieval tasks are typically solved with a two-stage approach: engineered features are extracted from music audio signals, and are then used as input to a regressor or classifier.
Although that approach was dominant in the past, feature learning and end-to-end learning (from signal to label) have started to receive more attention from the MIR community in recent years. 
The primary goal of this dataset is to enable researchers to train large models, which require lots of training data, which could potentially avert the semantic gap currently observed between the extracted low-level features such as MFCCs and the high-level targets such as genre.

This paper describes the dataset and how it was created, proposes some tasks and evaluate some baselines for music genre recognition (MGR).
All the code to reproduce it as well as links to download the data and usage examples are available at \url{https://github.com/mdeff/fma}.


%%%%%%%%%%%%%%%%%%%
\section{Introduction} % and Motivation}
%%%%%%%%%%%%%%%%%%%

\noindent
{\bf Music Analysis Field.} Developing new mathematical models and algorithms to solve challenging real-world problems is obviously of first importance in any field of research. But such novel techniques must be evaluated and compared to the existing state-of-the-art techniques to be adopted as new standards by research communities. Evaluation and comparison require benchmark datasets to achieve their purpose, which are sometimes challenging to collect and share. In computer vision, the community has developed over the years well-established benchmark datasets such as \href{http://yann.lecun.com/exdb/mnist/}{MNIST} \cite{mnist}, \href{https://www.cs.toronto.edu/~kriz/cifar.html}{CIFAR} \cite{cifar}, or \href{http://www.image-net.org}{ImageNet} \cite{imagenet}. All these image datasets are free, legal and easily available on Internet. Such datasets have proved essential to advance the field of computer vision. The most celebrated example is the ImageNet dataset and challenge in 2012. This unprecedented dataset of 1.5M image data allowed to demonstrate the power of deep learning techniques, which was able to win the competition by a large margin over the second best. In music analysis, such benchmark datasets are not easily available and they also lack of essential information. For examples, developing deep learning techniques for music applications require the use of raw audio data, and designing recommender systems need meta-data like artist features and song ratings. Besides, the most influential competition in the field of music analysis organized every year is the Music Information Retrieval Evaluation eXchange (MIREX).\footnote{\url{http://www.music-ir.org/mirex/wiki/}} MIREX proposes several important music analysis challenges such as song identification, tag classification, music similarity and retrieval, etc. However, participants do not have access to neither the test set, nor the important train set. They must upload their code to a website that will be evaluated by the organizers. In other words, participants cannot train their analysis models on any part of the dataset. These existing shortcomings make essential the development of new music datasets to advance the field of music analysis.   \\

In contrast with other modalities such as images or text for which a wealth of content is available, the lack of a large-scale and easily available dataset for MIR has hindered research on data-heavy models such as Deep Learning (DL).

\noindent
{\bf Available music datasets and their limitations.} In music analysis, three datasets have been widely used by the international society for music information retrieval (ISMIR)\footnote{\url{http://www.ismir.net/}}: the GTZAN, TagATune, and MSD (Million Song Dataset) datasets. However each dataset come with some limitations:
We discuss below some of the most related datasets, see \tabref{tab:datasets} for a more comprehensive list.
\tabref{tab:datasets} shows the most common datasets used for audio-based / content-based MIR.
\begin{description}
	\item[GTZAN] \cite{gtzan} is a collection of 1000 songs with 10 music genres. Each song is represented by a 22,050Hz Mono 16-bit wav audio file of 30sec. It is the first benchmark MGR dataset to have been made publicly available, and as a result continues to be the most used public dataset for MGR \cite{mgr_eval}. A complete study, and critic, of this dataset was done in \cite{gtzan_critic_1}, and the dataset is available online. The main limitations of GTZAN is the legality of the dataset, the small size, no complete meta-data regarding artist names and song titles, and no additional meta-data like ratings.
	\item[MagnaTagATune] \cite{magnatagatune} is a popular dataset of 5,405 source songs from 230 artists, cut in 25,863 clips. The music was collected from the \href{https://magnatune.com/}{Magnatune} label and was tagged using the \href{http://tagatune.org/}{TagATune} game. The dataset includes meta-data, audio features and raw audio, although the 16 kHz, 32kbps, mono mp3 are of poor audio quality.
	\item[The Million Song Dataset (MSD)] \cite{msd} is a free and legal collection of 1 million music songs. Each song data is composed of high-level and medium-level audio features provided by  Echonest service and meta-data like artist name, song title, track genre, etc. It has its own dedicated website with related code and additional datasets. The very large scale of this music dataset would make it an ideal candidate for deep learning, which works best with large datasets. However, deep learning techniques work directly on the raw data (audio tracks in this case, like raw image pixels in computer vision) and not on pre-processed audio features given by Echonest. This prevents the application of deep learning on this dataset. Finally, Echonest reassigned the indexing of the MDS songs with its database, which henceworth makes (almost) impossible the use of the most music recent features developed by Echonest.
		While researchers have been able to download sample audio from the online service \url{7signal.com} and extract features by themselves, the process is tedious \cite{msd_features}.
	\item[AudioSet] \cite{audioset} is a dataset of 2.1 million annotated 10-second sound clips associated with 527 classes. The ontology is quite broad and covers sounds from music to cars, engines or animals. The raw audio is however not included and has to be downloaded from Youtube. It is not clear of what happens if a referenced video gets deleted.
\end{description}

\begin{table}
	\centering
	\begin{threeparttable}
	\begin{tabular}{lrrcc}
		\toprule
		dataset & \#clips & \#artists & year & audio \\
		\midrule
		\href{https://staff.aist.go.jp/m.goto/RWC-MDB/}{RWC} \cite{RWC} & 465 & - & 2001 & yes \\
		\href{http://calab1.ucsd.edu/~datasets/cal500/}{CAL500} \cite{cal500} & 500 & 500 & 2007 & yes \\
		\href{http://mtg.upf.edu/ismir2004/contest/tempoContest/node5.html}{Ballroom} \cite{ballroom} & 698 & - & 2004 & yes \\
		\href{http://ismir2004.ismir.net/genre_contest/}{ISMIR2004} & 729 & - & 2004 & yes \\
		\href{https://marsyasweb.appspot.com/download/data_sets/}{GZTAN} \cite{gtzan} & 1,000 & $\sim300$ & 2002 & yes \\
		\href{http://www.cp.jku.at/datasets/musiclef/}{MusiClef} \cite{musiclef} & 1,355 & 218 & 2012 & yes \\
		\href{https://labrosa.ee.columbia.edu/projects/artistid/}{Artist20} \cite{artist20} & 1,413 & 20 & 2007 & yes \\
		\href{http://www-ai.cs.uni-dortmund.de/audio.html}{Homburg} \cite{garageband} & 1,886 & - & 2005 & yes \\  % GarageBand
		\href{http://www.seyerlehner.info/index.php?p=1_3_Download}{Unique} & 3,115 & - & 2010 & no \\
		\href{http://www.seyerlehner.info/index.php?p=1_3_Download}{1517-Artists} & 3,180 & 1,517 & 2010 & yes \\
		\href{http://www.ppgia.pucpr.br/~silla/lmd/}{LMD} \cite{lmd} & 3,227 & - & 2007 & no \\
		\href{http://anasynth.ircam.fr/home/media/ExtendedBallroom}{EBallroom} \cite{extballroom} & 4,180 & - & 2016 & no\tnote{1} \\
		\href{https://labrosa.ee.columbia.edu/projects/musicsim/uspop2002.html}{USPOP} \cite{uspop} & 8,752 & 400 & 2003 & no \\
		\href{http://calab1.ucsd.edu/~datasets/cal10k/}{CAL10k} \cite{cal10k} & 10,271 & 4,597 & 2010 & no \\ % Swat10k
		\href{http://mirg.city.ac.uk/codeapps/the-magnatagatune-dataset}{TagATune} \cite{magnatagatune} & 25,863\tnote{2} & 230 & 2009 & yes\tnote{3} \\ % MagnaTagATune
		\href{http://jmir.sourceforge.net/index_Codaich.html}{Codaich} \cite{codaich} & 26,420 & 1,941 & 2006 & no \\ % paper 20,849
		\bf \href{https://github.com/mdeff/fma/}{FMA} & \bf 110,000 & \bf - & \bf 2017 & \bf yes \\
		\href{http://www.omras2.org/}{OMRAS2} \cite{omras} & 152,410 & 6,938 & 2009 & no \\
		\href{https://labrosa.ee.columbia.edu/millionsong/}{MSD} \cite{msd} & 1,000,000 & 44,745 & 2011 & no\tnote{1} \\
		\href{https://research.google.com/audioset/}{AudioSet} \cite{audioset} & 2,100,000 & - & 2017 & no\tnote{1} \\
		\bottomrule
	\end{tabular}
	\begin{tablenotes}
		\item[1] Audio can be downloaded from a web service. % \url{www.ballroomdancers.com} \url{7signal.com} \url{youtube.com}
		\item[2] The 25,863 clips come from 5,405 songs.
		\item[3] Low quality 16 kHz, 32kbps, mono mp3.
	\end{tablenotes}
	\end{threeparttable}
	\caption{Comparison with some other datasets.}
	\label{tab:datasets}
\end{table}

% Lists of datasets:
% * http://www.audiocontentanalysis.org/data-sets/
% * A Survey of Evaluation in Music Genre Recognition

% TODO licensing

\begin{table}
	\centering
	\begin{threeparttable}
	\begin{tabular}{lrrcc}
		\toprule
		dataset & \#clips & length\tnote{1} & \#days\tnote{2} & size\tnote{3} \\
		\midrule
		\href{http://mirg.city.ac.uk/codeapps/the-magnatagatune-dataset}{TagATune} \cite{magnatagatune} & 25,863 & 29 & 8.7 & 3 \\ % MagnaTagATune
		\href{https://research.google.com/audioset/}{AudioSet} \cite{audioset} & 2,100,000 & 10 & 243 & - \\
		\href{https://labrosa.ee.columbia.edu/millionsong/}{MSD} \cite{msd} & 1,000,000 & 30, 60\tnote{4} & 541 & 625 \\
		\bf \href{https://github.com/mdeff/fma/}{FMA} & \bf 110,000 & \bf 10-180 & \bf ? & \bf 900 \\
		\bottomrule
	\end{tabular}
	\begin{tablenotes}
		\item[1] Length of available clips in seconds.
		\item[2] Number of days to listen to the whole available audio.
		\item[3] Size of the audio (accounts for duration and quality).
		\item[4] Most excerpts from 7digital are 30 or 60s \cite{msd_features}.
	\end{tablenotes}
	\end{threeparttable}
	\caption{Audio size comparison.}
	\label{tab:size}
\end{table}

% table quality

Our differentiating factors:
\begin{description}
	\item[Permissive licensing.] MIR research has historically suffered from the lack of publicly available benchmark datasets. Most of these issues stem from the commercial interest in music by record labels, and therefore imposed rigid copyright. All songs are licensed under Creative Commons by the artists and can thus be redistributed.
* In contrast with images or text for which plenty of data is available online
	\item[Audio.] most of the largest datasets are distributed with features, sometimes computed by commercial services like the Echonest. Researchers are wary of those proprietary features and have seek to compute features themselves, e.g. on the MSD \cite{msd_features}. In contrast, FMA provides audio for each song.
	\item[High quality audio.] full-length original audio uploaded by the artist. Stereo mp3 with bitrates varying between x. Audio, if released or available to download, is usually clips of 10 to 30 seconds. To our knowledge, none of the released datasets up to this date come with full-length audio.
	\item[Future proof]: all audio and meta-data can be downloaded as ZIP archives, no need to scrap the web yourself. They are checksummed, have a DOI and are hosted on \href{https://zenodo.org}{zenodo.org}, a long-term digital archive powered by CERN. We thus alleviate the risks of tracks to become unavailable, to change without notice or for online services to shut down.
	% Western / commercial music, how to qualify broadness?
	\item[Reproducible]: we share all the code used to (i) collect the dataset, (ii) analyze it, (iii) extract the sub-sets, (iv) compute the features and (v) the baselines, so that it can be reproduced or extended. As the original data and APIs are public, anybody can recreate the dataset. A train / test split is provided to make research on the FMA reproducible.
	\item[Easy to work with]: working with the dataset only requires to download some archives. Moreover, we provide an helper Python package\footnote\url{https://pypi.python.org/pypi/freemusicarchive} and usage examples as Jupyter notebooks to quick start using the data.
\end{description}

% Magnatagature dead links: http://tagatune.org/Datasets.html http://tagatune.org/Magnatagatune.html
% Announcement: https://musicmachinery.com/2009/04/01/magnatagatune-a-new-research-data-set-for-mir/

% TODO: talk about related datasets
% * SecondHandSongs dataset -> cover songs
% * musiXmatch dataset -> lyrics
% * Last.fm dataset -> song-level tags and similarity
% * Taste Profile subset -> user data
% * thisismyjam-to-MSD mapping -> more user data
% * tagtraum genre annotations -> genre labels
% * Top MAGD dataset -> more genre labels

The sets in \tabref{tab:datasets} are either smaller, either provide audio through online services.

From \tabref{tab:datasets}, it is obvious that the main problem is that larger datasets do not come with the audio. Either they contain only features derived from the audio, either they provide links for the user to download the audio from an online service. The problem with the former is that researchers are stuck with the set of chosen features, while the later make no assurance that the files or services are not gonna disappear or change without notice.
We see from \tabref{tab:datasets} that most of the smallest datasets come with audio while most of the largest don't. That state of affair is due to copyright laws which forbid the redistribution of audio. The solution to that problem is to aim for tracks released under permissive licenses, such as Creative Commons\footnote{\url{https://creativecommons.org/}}.

Creative Commons: MagnaTagATune
Proprietary, non-redistributable: MSD, AudioSet?

\noindent
{\bf What would be the requirements for an ideal music dataset?}  \\
(1) Availability of raw audio tracks like 30sec or more,\\
(2) Availability of meta-data such as genres, artist, title, year, lyrics, users' ratings, users' comments, etc,\\
(3) Large-scale dataset to be able to sample the high-dimensional distribution of music genres\\
(4) Free and easy access of the dataset,\\
(5) Legality of the dataset, no copyright issue with e.g. open access licence.\\





%%%%%%%%%%%%%%%%%%%
\section{The Dataset} %Introducing the FMA dataset}
%%%%%%%%%%%%%%%%%%%

% Describe what it is, what are the meta-data and features.

% TODO audio properties: samplerate (% 44kHz), bitrate (% 128, VBR), channels (% mono, stereo)
Most tracks are released under a Creative Commons license\footnote{\url{https://creativecommons.org/}} which allows redistribution of the audio.

\subsection{Analysis}
% Figures and tables from analysis.ipynb

\subsection{Features} % Feature sets

Extracted with librosa\footnote{\url{https://github.com/librosa/librosa}}.

In addition, a number of descriptive features extracted with the services from The Echo Nest are provided. These features include tempo, loudness, timings of fade-in and fade-out, and MFCC-like features for a number of segments.

\subsection{Collection} % Creation, Generation
% How it was collected.

\noindent
{\bf The website.} The FMA website\footnote{\url{https://freemusicarchive.org/}} is run by WFMU, the longest-running free-form radio station in the United States. The website provides a large catalogue of artists and high-quality mp3 songs. Each song is legally free to download as artists decided to work under open licence such as Creative Commons license. This is directly inspired by the open source movement, and the goal of the FMA site is to offer a legal and technological platform for artists and listeners to harness the full potential of online music sharing \cite{art:MossFMA}. FMA has been funded by the New York State Music Fund, the MacArthur Foundation, and the National Endowment for the Arts.\\



\noindent
{\bf Crawling the data.} The FMA organization provides an API\footnote{\url{https://freemusicarchive.org/api}} to collect the meta-data associated to each song such as artist name, track title, track genre, and track listens. The collect of the mp3 audio files was done with a web scrapper. The collection of FMA was done in April 2016, and the size of the dataset at this time was 89,912 songs. It was recently announced on July 2016 that the website had reached the landmark of 100,000 songs. \\



\noindent
{\bf The raw FMA dataset.} The dataset collected on April 2016 contains 89,912 music data. As a first filtering, we kept the data that has audio tracks, reducing the size to 86,886 music data. All audio tracks are mp3-encoded with sampling rate of 44,100Hz, bit rate 128kb/s, and in stereo. Then, we considered the meta-data provided by the FMA website and crawled with its API. The meta-information collected is artist name, track title, track genres, and track listens. We removed data without genre information, giving 79,947 song data. Then, we created a top genre by simply picking the first genre in the original list of track genres provided by the artists. A few examples of data are given in Table \ref{tab1}. At this stage the number of genres is 138, but half have only a few samples. Thus we decided to only retain the genres with more than 100 samples. This defines a dataset of 77,643 music data with 68 genres. Figure \ref{fig_large} presents the distribution of the top 20 genres, which obeys to a power law distribution as often encountered in natural data collection. We further cleaned the dataset by removing tracks which belong to ``non-standard'' music genres like 'Noise', 'Garage', 'Sound Collage', 'Singer', 'Audio Collage', 'Glitch', 'Unclassifiable', 'Lo-Fi', 'Spoken', 'Poetry', 'Talk Radio', 'Avant-Garde', 'Experimental', 'Ambient', and 'Field Recording'. We also suppressed the songs with the title 'Untitled'. Besides, we decided to cut the long tail of the power law distribution by removing songs of genres with less than 300 songs. Finally, we kept the songs that have the raw audio and all Echonest features. Echonest features are medium-level and high-level audio features provided by Echonest\footnote{\url{http://the.echonest.com/}}, a music platform for developers and media companies which has a collection of 3.5M artists and 37.5M songs. Echonest features can be either computed by uploading the songs to the Echonest website, which are then analyzed and send them back to the user, or simply retrieved from the Echonest database if the songs were analyzed before. All Echonest operations are done through an API\footnote{\url{http://developer.echonest.com}}. Our final filtering consists in keeping songs that have both raw audio file and complete Echonest features already computed and stored in the Echonest database. The final size of the dataset is 14,511 music data and 20 genres. Figure \ref{fig_medium} presents the distribution of the genre sizes for this dataset. \\

  
 
\noindent
{\bf Proposed benchmark datasets.} For research purposes, we propose 3 datasets:\\
(1) FMA\_small: This dataset contains 4,000 songs with 10 genres equally balanced, that is 400 songs per class. For each song, we extracted 30sec precisely in the middle of the song. The total length is 33.3 hours of audio listening and the size is 3.4GB. The genre distribution is given in Figure \ref{fig_small}. The motivation for this dataset is to create an alternative to the popular GTZAN dataset with respect to the size and genre parameters. But unlike GTZAN, all songs have not only raw audio tracks but also meta-data composed of artist name, track title, track genres, track listens, and Echonest features. The GTZAN data may not have for example song title or artist name, and also no copyright permission. For reproducibility research, we also provide a train-test split 80-20 of this dataset for future techniques' comparisons.   \\
(2) FMA\_medium: This dataset has 20 genres with 14,511 songs of 30sec, also extracted from the middle of the track. The total length is 5.1 days of listening and the size is 12.2GB. Unlike FMA\_small, the size distribution of genres is not balanced, see Figure \ref{fig_medium}, but all songs have raw audio files and all meta-data i.e. artist name, track title, track genres, track listens, and Echonest features. We also provide a split 80-20 of this dataset.  \\
(3) FMA\_large: This dataset has 77,643 data of 30sec, again taken from the track middle, and 68 genres. It provides 26.9 days of track listening for 79.8GB. Figure \ref{fig_large} presents the top classes in the genre distribution. All songs come with the following meta-data: artist name, track title, track genres, track listens. Eventually, we give a split 80-20 of this dataset.\\


\noindent
Regarding the splitting train-test sets, we considered these constraints:\\
(a) For each genre, we randomly split train and test songs with the same ratio 80-20.\\
(b) We forced an artist to be either in the train set, or the test set (meaning the artist's songs cannot be in both train and test sets). \\
(c) We filled alternatively the train set and test set in the descending order of the count of artist songs. This prevented the test set to be filled with artists who have only a few songs.\\


\noindent
The FMA\_large dataset of 77,643 data contains songs of 30sec length, and artist name, track title, track genres, track listens as meta-data. Our original motivation was to give the full audio tracks, not limited to 30sec, but the size of the whole music tracks is 806.8GB (for approximatively 256 days of music listening) making the hosting a challenging issue.




\section{Proposed tasks} % usage, problems
% List some MIR tasks and propose some who can be done with that dataset

genre classification, mood classification, artist identification/recognition, instrument recognition, music annotation (tags),
Artist Identification, Genre classification, Mood Classification, Instrument identification, Music Similarity, Autotagging, Automatic playlist generation

Can be down: genre, artist, popularity prediction, tags, year prediction

% TODO
% Problems
% Genre classif / mutli-genre classif
% Recommendation
% Play count prediction
% Covers: generate covers per genre / artist

Meta-data analysis: which genres are artists contributing to, drift in time, curators.
It would also be interesting to study how good does it represent mainstream music.

As it happened with e.g. the MSD, people may find and link other sources of data with the dataset, such as lyrics or tags associated to the songs, e.g. from last.fm.

We propose three genre classification problems of varying difficulty.

\begin{enumerate}
	\item Easy, based on \texttt{fma\_small.zip}: 10 top-level genres, balanced with 400 tracks per genre, 1 genre per track
	\item Medium, based on \texttt{fma\_medium.zip}: all 16 top-level genres, unbalanced with x-y tracks per genre, multiple genres per track
	\item Hard, based on \texttt{fma\_large.zip}: all 164 genres, unbalanced with x-y tracks per genre, multiple genres per track
\end{enumerate}

Training / testing division.


\section{Genre Recognition} % MGR, Baselines
% One of the most common task, we give some baselines so that people have a reference about how hard it is.

Music genre recognition (MGR) is one of the most researched areas of MIR.

%\noindent
%{\bf Baseline classification techniques.}  In this section, we consider perhaps the most popular music analysis problem, the music genre recognition (MGR) problem. We apply standard baseline classification techniques on 2 representations of audio data: \\
%(1) Echonest medium-level 256-dim features, \\
%(2) Spectrograms of audio data by concatenating 10 spectrograms of 3sec into a single long vector.\\
%Table xxx reports the classification accuracy on the train and test datasets.\\
%
%
%\noindent
%{\bf Deep learning.} One major motivation to construct the FMA dataset was to apply the powerful deep learning techniques to the music analysis problem. As a preliminary result, we simply apply a convolutional neural network on the 3sec-spectrograms, and we select the class by majority voting. \\
%Table xxx reports the classification accuracy on the train and test datasets.\\




%%%%%%%%%%%%%%%%%%%
\section{Conclusion and Perspectives}
%%%%%%%%%%%%%%%%%%%

The FMA enables researchers to test algorithms on a large-scale collection, thus allowing to test them on more real-world like environments. By providing audio, we do not limit the benchmarking to pre-computed features and allow scientists to develop and test new feature sets, learn features, or learn mappings directly from the audio.

%\paragraph{Availability and reproducible research.}
{\bf Availability and reproducible research.}
All the datasets as well as Jupyter notebooks used to produce them, the
analysis, and baselines as are available online at

\url{https://github.com/mdeff/fma}

Code to support the use of the data is provided as well.
 
%\paragraph{Potential applications.}
{\bf Potential applications.}
We foresee multiple applications of this new dataset. The most obvious one is for music analysis with deep learning techniques. As raw music tracks are available, deep learning techniques like convolutional neural networks \cite{mnist} and recurrent neural networks \cite{art:HochreiterSchmidhuber97LSTM} can be directly applied. Besides, standard data analysis tasks such as classification, clustering, regression, text analysis, visualization can also be applied to this new music dataset.

While this data release is pretty large, it is still two orders of magnitude behind commercial services who have access to tens of millions of songs.\footnote{\href{http://the.echonest.com}{37M Echonest}, \href{https://en.wikipedia.org/wiki/Spotify}{30M Spotify}, \href{http://www.skilledtests.com/wiki/Last.fm_statistics}{45M last.fm}, \href{http://bupz.com/best-websites-to-buy-musics}{45M 7digital}, \href{https://www.apple.com/pr/library/2013/02/06iTunes-Store-Sets-New-Record-with-25-Billion-Songs-Sold.html}{26M iTunes}}

% last.fm: 45 million http://www.skilledtests.com/wiki/Last.fm_statistics

\section{Acknowledgments}

We are grateful to SWITCH and EPFL for hosting the dataset within the context
of the \href{https://projects.switch.ch/scale-up/}{SCALE-UP} project, funded in
part by the swissuniversities \href{http://www.swissuniversities.ch/isci}{SUC
P-2 program}.

We are grateful to the team supporting the \href{https://freemusicarchive.org}{Free Music Archive} as well as all the contributing artists and curators for the fantastic content they made available.


\bibliography{refs}





\begin{figure}[h!]
\centering
\includegraphics[height=6cm]{histo_large.pdf}
\vspace{-0.5cm}
\caption{Top 20 music genres for the FMA\_large dataset of 77,643 songs.}
\label{fig_large}
\end{figure}






\begin{figure}[h!]
\centering
\includegraphics[height=6cm]{histo_medium.pdf}
\vspace{-0.5cm}
\caption{The 20 music genres for the FMA\_medium dataset with 14,511 songs.}
\label{fig_medium}
\end{figure}


\begin{figure}[h!]
\centering
\includegraphics[height=5cm]{histo_small.pdf}
\vspace{-0.5cm}
\caption{The 10 music genres for the FMA\_small dataset with 4,000 songs.}
\label{fig_small}
\end{figure}





%\begin{table}[h!]
\begin{sidewaystable}
\tiny{
\centering
\begin{tabular}{lrlrllll}
\toprule
 fma\_id &                                             artist &  play\_count &                                              title &                                             genres &            top\_genre  \\
 \toprule
    10 &                                          Kurt Vile &       42936 &                                            Freeway &                                              [Pop] &                  Pop  \\
    134 &                                               AWOL &         880 &                                       Street Music &                                          [Hip-Hop] &              Hip-Hop  \\
     141 &                    Alec K. Redfearn \& the Eyesores &         590 &                                               Ohio &                                             [Folk] &                 Folk  \\
     142 &                    Alec K. Redfearn \& the Eyesores &         670 &                               Punjabi Watery Grave &                                             [Folk] &                 Folk  \\
    144 &                                   Amoebic Ensemble &         901 &                                            Wire Up &                                             [Jazz] &                 Jazz  \\
   145 &                                   Amoebic Ensemble &         682 &                                         Amoebiasis &                                             [Jazz] &                 Jazz  \\
    188 &                                           Ed Askew &         973 &                                            Piano 1 &                                             [Folk] &                 Folk  \\
     206 &                                           Ed Askew &          73 &                                        Bella Crane &                                             [Folk] &                 Folk \\
     236 &                                       Banana Clipz &        6695 &                              Push Am (Left, Right) &                              [Electronic, African] &           Electronic  \\
     237 &                                          Barnacled &        1056 &                    Garbage and Fire &                                             [Jazz] &                 Jazz  \\
     238 &                                          Barnacled &         961 &                                     France Attacks &                                             [Jazz] &                 Jazz  \\
     461 &                              Cantonement Jazz Band &        3933 &                                           Bessemer &                               [Blues, Jazz: Vocal] &                Blues  \\
     462 &                              Cantonement Jazz Band &        3256 &                                     Has Been Blues &                               [Blues, Jazz: Vocal] &                Blues  \\
     463 &                              Cantonement Jazz Band &        3238 &                                       I'll Be Blue &                               [Blues, Jazz: Vocal] &                Blues  \\
    824 &                     Here Comes A Big Black Cloud!! &         391 &                                         Black Mold &          [Rock, Loud-Rock, Psych-Rock, Indie-Rock] &                 Rock  \\
     825 &                     Here Comes A Big Black Cloud!! &        1847 &                                        Death March &          [Rock, Loud-Rock, Psych-Rock, Indie-Rock] &                 Rock  \\
    837 &                                          Heroin UK &         146 &                                           DopeSick &                                             [Rock] &                 Rock \\
     889 &                                 Illusion of Safety &         163 &                                          Wasteland &                                       [Electronic] &           Electronic  \\
     896 &                                        Impediments &        1542 &                                               2012 &                                  [Punk, Power-Pop] &                 Punk  \\
          992 &                                      Jason Willett &         560 &                   Beautiful Song w/ kick drum solo &                                             [Rock] &                 Rock  \\
\bottomrule
\end{tabular}
\caption{A few song data in the FMA dataset.}
\label{tab1}
%\end{table}
}
\end{sidewaystable}



\end{document}
